{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af4577d-c641-441f-8fd4-e56dd6ab8774",
   "metadata": {},
   "source": [
    "# Train sensitivity model with GMM VAE as DVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa53525-0150-462e-aec5-68f14544ba41",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c721898-5dd5-45a0-8a9f-710a0309ec7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%run ./utils/imports.py\n",
    "\n",
    "import utils.utils as utils\n",
    "from models import GMMVAE, SensitivityModelGMMVAE, modules\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/adam/Projects/vadeers/code/gmm-vae-compounds/models/hgraph2graph/')\n",
    "\n",
    "import rdkit.Chem as Chem\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cc3e7b-5a56-42f3-ae87-b7d7b87b42d4",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a92253-22d7-42ff-9c9c-6c4bd643d6ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General path\n",
    "dataset_dir = \"/home/adam/Projects/vadeers/data/Ready Datasets/Baseline Dataset/\"\n",
    "\n",
    "# Sensitivity table\n",
    "sensitivity_table = pd.read_csv(os.path.join(dataset_dir, \"sensitivity_table.csv\"))\n",
    "\n",
    "# Cell lines biological data\n",
    "cell_lines_biological_data = pd.read_csv(os.path.join(dataset_dir, \"cell_lines_biological_data_from_deers.csv\"))\n",
    "\n",
    "# Drugs SMILES vector representations\n",
    "drugs_mol2vec_reprs = pd.read_csv(os.path.join(dataset_dir, \"drugs_Mol2Vec_reprs.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c0a652-98cd-40ec-a48c-020c6428afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load appropriate data\n",
    "\n",
    "# Load drugs inhibition profiles\n",
    "NO_TRUE_CLUSTER_LABELS = 3\n",
    "drugs_inhib_profiles= pd.read_csv(\"/home/adam/Projects/vadeers/data/Ready Datasets/Baseline Dataset/drugs_inhib_profiles_with_3_guiding_cluster_labels.csv\")\n",
    "\n",
    "# Create mappers from IDs to indexes\n",
    "cell_line_ID_to_index_mapper = utils.get_ID_to_idx_mapper(cell_lines_biological_data, id_col=\"cell_line_id\")\n",
    "drugs_ID_to_smiles_rep_index_mapper = utils.get_ID_to_idx_mapper(drugs_mol2vec_reprs, id_col=\"PubChem CID\")\n",
    "drugs_ID_to_inhib_profiles_index_mapper = utils.get_ID_to_idx_mapper(drugs_inhib_profiles, id_col=\"PubChem CID\")\n",
    "\n",
    "# Create main dataset\n",
    "full_dataset = utils.DatasetThreeTables(sensitivity_table, \n",
    "                                        cell_lines_biological_data.values[:, 1:], \n",
    "                                        drugs_mol2vec_reprs.values[:, 1:], \n",
    "                                        drugs_inhib_profiles.values[:, 1:],\n",
    "                                        cell_line_ID_to_index_mapper, \n",
    "                                        drugs_ID_to_smiles_rep_index_mapper, \n",
    "                                        drugs_ID_to_inhib_profiles_index_mapper,\n",
    "                                        drug_ID_name=\"PubChem CID\", \n",
    "                                        cell_line_ID_name=\"COSMIC_ID\", \n",
    "                                        guiding_data_class_name=\"guiding_data_class\",\n",
    "                                        sensitivity_metric=\"LN_IC50\", \n",
    "                                        drug_ID_index=1, \n",
    "                                        cell_line_ID_index=3, \n",
    "                                        sensitivity_metric_index=4)\n",
    "\n",
    "# Create VAE dataloader\n",
    "VAE_BATCH_SIZE = 8\n",
    "vae_dataset = utils.get_vae_dataset(drugs_mol2vec_reprs, drugs_inhib_profiles)\n",
    "vae_dataloader = DataLoader(vae_dataset, batch_size=VAE_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([ 1.0462272e+00, -3.2771523e+00, -6.3824110e+00,  1.8789635e+00,\n        -7.9489590e-01, -5.8907150e+00, -8.2104870e+00,  3.5316072e+00,\n         1.5746067e+00, -1.0245272e+00,  1.1576798e+00,  1.2552512e+00,\n        -1.4733623e+00, -3.3957162e-01, -2.3549979e+00, -4.5432060e+00,\n         2.7046566e+00, -1.7262821e+00, -6.2296480e+00,  9.3133620e+00,\n         6.7901673e+00,  8.7795410e+00,  1.2105804e+01,  7.7198935e+00,\n        -8.0074170e+00,  3.9709947e+00, -1.4577461e+00, -6.6870420e+00,\n         1.7495483e-02, -4.4180405e-01,  8.2790650e+00, -4.8904943e+00,\n        -4.6256530e+00, -7.1233892e+00,  2.5176482e+00, -4.1075134e-01,\n        -3.1874676e+00, -3.3411288e-01,  1.1363386e+01,  3.2968955e+00,\n        -5.5781290e-01,  1.4544309e+00, -1.7068732e+00, -9.8873830e-01,\n        -9.4704820e+00,  1.0955430e+01,  8.6190460e-01,  9.6832680e+00,\n        -4.9702187e+00,  2.5784788e+00,  3.8157666e+00, -1.1041448e+01,\n         3.4110174e-01, -2.4691017e+00, -1.5018735e+01, -5.6186547e+00,\n        -7.4649010e+00,  2.1169798e+00,  2.8473723e+00,  4.9603290e+00,\n        -7.9964520e+00, -6.9367860e+00, -3.8898106e+00,  6.4437560e+00,\n        -1.4781246e+01, -5.1616190e-01,  1.3369509e+01, -5.4056212e-02,\n        -6.7338176e+00, -3.2017160e+00, -7.4786544e+00,  6.4272003e+00,\n         8.3299010e+00, -6.0592140e+00, -2.2598680e+00,  1.8492755e+00,\n        -2.8821523e+00,  5.1837770e+00, -4.5440574e+00,  2.1769168e+00,\n         5.0882645e+00, -5.1328354e+00, -5.6083727e+00, -1.4779234e+01,\n         3.0681086e+00, -5.6765880e-01, -8.8589040e+00,  6.8925200e+00,\n         5.1895056e+00,  6.5591190e-02,  9.2953290e-03,  3.4215060e+00,\n        -3.1186390e+00,  5.3477967e-01,  4.6488630e+00, -3.0796520e+00,\n         5.1267076e+00,  3.4311526e+00,  4.1644730e+00, -1.8557560e+00,\n        -5.0935054e+00, -8.9021400e+00, -5.3481836e+00, -5.7998820e+00,\n         1.0088050e+00, -6.7184052e+00, -1.8859339e-01,  7.1725880e+00,\n         3.7117300e+00, -7.4539332e+00, -1.1566351e+01,  2.8486870e+00,\n        -2.0720267e-01, -1.9502639e+00,  8.5345020e-02, -4.5804415e+00,\n         8.2352870e+00,  1.0270445e+01, -7.2995677e+00, -2.1092756e+00,\n        -5.5333580e+00,  2.2427397e+00,  1.2601159e+00,  1.1534529e+01,\n         4.4494650e+00, -8.0599690e+00, -8.1884520e+00, -6.3954943e-01,\n        -3.4365258e+00, -1.6944473e+00, -3.5004582e+00,  2.3251336e+00,\n         9.9168415e+00,  9.8728080e+00,  6.2261260e+00,  1.2776326e+01,\n         9.4700300e+00,  1.0145476e+00,  3.8228545e+00, -7.0546846e+00,\n         4.9814520e+00, -1.6996441e+00, -3.5262597e+00, -1.6449677e+00,\n         4.3041167e+00,  1.1175442e+01, -4.5879464e+00,  1.8833748e+00,\n         5.7449803e-02,  3.4595480e+00,  6.2688518e+00,  5.9342020e+00,\n         6.1557440e+00, -6.4095473e-01,  1.7782813e+00,  4.7550540e+00,\n        -6.5082655e+00, -2.0465407e-01,  3.5438044e+00, -2.1510160e-01,\n        -2.8357272e+00, -1.5807262e-01, -3.5924560e+00, -7.7596500e-01,\n         2.7202873e+00, -5.8855405e+00, -2.3263104e+00, -5.2553300e+00,\n        -2.2469914e+00,  3.2363663e+00, -3.6931179e+00, -2.0098870e+00,\n         6.9945400e+00, -3.4809573e+00,  2.6634645e+00, -3.8479435e+00,\n         4.5551596e+00, -7.8552260e-01, -3.4443655e+00,  1.1413288e+01,\n        -1.8820841e+00,  7.4854930e+00,  2.2010427e+00,  1.3504502e+01,\n         9.6269270e+00, -3.0472941e+00, -2.4541492e+00, -8.8290703e-01,\n        -2.3043510e+00, -6.5435653e+00, -6.2223970e-03,  2.7500038e+00,\n        -1.3778687e+00, -3.1907613e+00, -1.5117314e+00, -4.1666433e-02,\n        -7.5086430e+00, -9.6206280e+00,  1.4876105e+00,  4.1597268e-01,\n         3.1905174e+00,  1.3336143e+00,  4.8420150e+00,  4.8898770e+00,\n         3.8954747e+00,  5.4073367e+00,  5.9970617e+00,  5.9760513e+00,\n         1.2079181e+01, -1.1840072e+00,  1.0057689e+01, -5.7241460e+00,\n         9.1846090e+00, -7.7668447e+00,  3.7311928e+00,  1.2938442e+00,\n         2.0727453e+00,  9.0204040e+00, -6.1097670e+00,  2.8879988e+00,\n         3.7039232e+00, -1.2552858e+01, -8.2967770e+00,  4.0234100e+00,\n         1.5261371e+00, -9.3655270e+00, -2.6357872e+00, -5.4813013e+00,\n         6.4898596e+00, -2.5467010e+00,  4.3383120e+00, -8.6160610e-01,\n        -4.2309303e+00, -1.1898364e+01, -2.0601263e+00, -8.4302560e+00,\n         3.3228166e+00,  1.4466090e+00, -3.6957898e+00,  6.4092540e+00,\n         3.4752374e+00, -5.4251523e+00,  6.6756253e+00,  2.7791870e+00,\n        -4.6422750e+00,  8.0138870e+00, -8.9276470e+00, -8.5143460e+00,\n         2.2091874e-01, -1.5007033e+00, -1.3963427e+01,  7.0241950e+00,\n         2.4610622e+00,  5.3559055e+00, -1.5087310e+01,  3.1730506e+00,\n         1.8182498e+00, -4.3384905e+00,  7.0073853e+00, -6.3027353e+00,\n        -3.1779020e-01,  7.0202060e+00,  2.6488745e+00,  8.8025710e+00,\n         1.1628662e+00, -7.2033234e+00, -5.2370390e+00,  2.5097682e+00,\n         2.6525870e+00, -4.1604775e-01,  3.5905004e+00,  9.3905520e+00,\n         1.8353016e+00, -7.9661270e-01, -3.3333063e+00, -3.4623303e+00,\n        -1.9436846e+00,  1.9095365e+00, -1.0299984e+00,  5.4789476e+00,\n        -4.1092486e+00, -6.5153813e+00, -1.2618600e+01, -8.5135210e+00,\n         1.2287587e+00,  6.8275730e+00, -4.1520715e+00,  1.1145296e+00,\n        -3.5656980e+00,  2.3354135e+00, -1.4842120e+00,  1.3010543e+01,\n         2.9928765e+00, -3.2030278e-01, -1.2947478e+01, -9.8863930e+00,\n        -9.4723650e+00, -3.2952576e+00, -1.2529826e+01,  1.5407397e+00]),\n array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n        nan, nan, nan, nan, nan, nan, nan, nan]),\n nan,\n array([5.95771287, 3.10035722, 6.0159545 , 5.61268624, 4.55620324,\n        3.97668315, 3.33797298, 0.        , 7.38526301, 6.53671895,\n        3.02165254, 3.86891013, 4.41620844, 3.24599844, 4.59753361,\n        2.90001927, 3.7371972 , 3.77257275, 4.92275115, 4.59494157,\n        5.50419977, 0.        , 3.65273195, 0.        , 3.2430621 ,\n        7.63713986, 3.3381989 , 3.5701751 , 7.96084286, 3.07901322,\n        8.22233214, 3.89051115, 8.750848  , 6.77552448, 4.69732826,\n        3.7719364 , 3.50634529, 4.26382023, 4.93654722, 0.        ,\n        3.83568626, 3.62717136, 4.08707515, 4.6619207 , 5.14518965,\n        0.        , 3.28486634, 7.36649433, 0.        , 3.72612686,\n        3.53757707, 5.58359446, 6.54507902, 3.43418313, 4.05796205,\n        4.8342345 , 3.26882656, 7.12577939, 7.20111838, 0.        ,\n        6.04144946, 3.94888342, 3.16089064, 9.50695308, 2.72240146,\n        2.62496411, 3.11654408, 3.50361391, 3.01798619, 7.24874494,\n        6.34779006, 4.01826978, 0.        , 7.28376287, 7.82397999,\n        3.61976693, 7.11881951, 3.36973607, 5.17776524, 9.25208536,\n        8.65598351, 4.56488904, 3.60974192, 4.12170381, 3.396583  ,\n        6.55293335, 5.51665451, 6.97818705, 4.62559908, 5.92083443,\n        3.05260937, 3.80457656, 3.13395732, 3.07621667, 3.46332718,\n        7.64733829, 8.77057493, 3.18105858, 2.74194238, 3.30102414,\n        2.89449362, 0.        , 4.20940252, 7.19304072, 0.        ,\n        4.32524517, 6.17148657, 3.77064246, 3.40277772, 6.36301077,\n        3.20791652, 9.11623611, 0.        , 2.89405975, 3.81409195,\n        3.27030532, 3.90446169, 8.20160454, 3.41715766, 9.7744039 ,\n        8.41669589, 3.45533875, 6.52474167, 5.41167817, 4.72866635,\n        3.06199251, 8.86505405, 6.9431646 , 0.        , 3.17940455,\n        4.9353719 , 3.29198496, 5.66264824, 4.65701724, 2.87685147,\n        3.05798164, 3.06822725, 4.01997809, 2.88158856, 4.67888203,\n        3.06849044, 0.        , 3.16401219, 3.72862441, 2.99478025,\n        3.06037894, 7.20740723, 7.60171585, 3.03919965, 9.02568862,\n        2.98302245, 3.41078179, 5.35577634, 4.9306594 , 7.90190406,\n        4.49011356, 3.35430307, 2.87691938, 4.11064872, 7.37189736,\n        3.00365836, 6.98528105, 3.2165734 , 3.0697393 , 0.        ,\n        3.8213574 , 7.50365229, 3.34225157, 3.01906395, 2.74971842,\n        4.03279083, 5.11136876, 5.32599408, 3.56343963, 8.05486748,\n        0.        , 6.87276999, 7.9044512 , 5.30881448, 5.92211441,\n        3.32893814, 7.51210632, 5.72124525, 0.        , 3.23711235,\n        0.        , 5.900095  , 3.48584356, 6.2276269 , 3.29889482,\n        7.36036733, 6.47694392, 3.5675621 , 2.97168747, 7.31683416,\n        4.74720722, 4.79262289, 8.95002038, 3.24241015, 3.8481244 ,\n        7.96335189, 5.75644387, 3.40716194, 3.53147012, 0.        ,\n        3.61471133, 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 1.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 4.73653522,\n        5.48795694, 8.38778288, 3.76479574, 4.58106315, 5.5524602 ,\n        3.19829205, 3.71106057, 8.44142527, 3.400311  , 9.32518105,\n        3.15217711, 9.10201901, 7.11299116, 0.        , 0.        ,\n        0.        ]),\n tensor([-0.0749]),\n 9863776,\n 683665)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "af638801-267d-4157-871b-1019bc3af248",
   "metadata": {},
   "source": [
    "## Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240932e3-3bf1-4bb5-a2ca-45aee28cbaa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sensitivity model with GMM VAE\n",
    "# Input dimensionalities\n",
    "DRUG_INPUT_DIM = 300\n",
    "DRUG_GUIDING_DIM = 294\n",
    "CELL_LINE_INPUT_DIM = 241\n",
    "\n",
    "# Latent spaces dimensionalities\n",
    "DRUG_LATENT_DIM = 10\n",
    "CELL_LINE_LATENT_DIM = 10\n",
    "\n",
    "# NN layers\n",
    "DRUG_ENCODER_LAYERS = (DRUG_INPUT_DIM, 128, 64, DRUG_LATENT_DIM)\n",
    "DRUG_INPUT_DECODER_LAYERS = (DRUG_LATENT_DIM, 64, 128, DRUG_INPUT_DIM)\n",
    "DRUG_GUIDING_DECODER_LAYERS = (DRUG_LATENT_DIM, 64, 128, DRUG_GUIDING_DIM)\n",
    "CELL_LINE_ENCODER_LAYERS = (CELL_LINE_INPUT_DIM, 128, 64, CELL_LINE_LATENT_DIM)\n",
    "CELL_LINE_DECODER_LAYERS = (CELL_LINE_LATENT_DIM, 64, 128, CELL_LINE_INPUT_DIM)\n",
    "\n",
    "# Set number of components in latent GMM\n",
    "NO_GMM_COMPONENTS = NO_TRUE_CLUSTER_LABELS\n",
    "\n",
    "# Transformation to apply before encoders output\n",
    "var_transformation = lambda x: torch.exp(x) ** 0.5\n",
    "\n",
    "# Establish config dict\n",
    "whole_model_config = {\"drug_latent_dim\": DRUG_LATENT_DIM,\n",
    "                        \"cell_line_latent_dim\": CELL_LINE_LATENT_DIM,\n",
    "                        \"no_gmm_components\": NO_GMM_COMPONENTS,\n",
    "                        \"components_std\": 1.,\n",
    "                        \"drug_encoder_layers\": (DRUG_INPUT_DIM, 128, 64, DRUG_LATENT_DIM),\n",
    "                        \"drug_input_decoder_layers\": (DRUG_LATENT_DIM, 64, 128, DRUG_INPUT_DIM),\n",
    "                        \"drug_guiding_decoder_layers\": (DRUG_LATENT_DIM, 64, 128, DRUG_GUIDING_DIM),\n",
    "                        \"cell_line_encoder_layers\": (CELL_LINE_INPUT_DIM, 128, 64, CELL_LINE_LATENT_DIM),\n",
    "                        \"cell_line_decoder_layers\": (CELL_LINE_LATENT_DIM, 64, 128, CELL_LINE_INPUT_DIM),\n",
    "                        \"vae_loss_function_weights\": (1., 1., 1., 1., 0.),\n",
    "                        \"vae_var_transformation\": \"standard\",\n",
    "                        \"optimizer\": \"adam\",\n",
    "                        \"learning_rate\": 0.0005,\n",
    "                        \"aen_reconstruction_weight\": 1.,\n",
    "                        \"sensitivity_prediction_weight\": 1.,\n",
    "                        \"l2_term\": 0.,\n",
    "                        \"pretraining_vae\": False,\n",
    "                        \"batch_size\": 128,\n",
    "                        \"mixed_training\": True,\n",
    "                        \"vae_training_num_epochs\": 100,\n",
    "                        \"vae_training_step_rate\": 1000,\n",
    "                        \"drug_model_learning_rate\": 0.0005,\n",
    "                        \"vae_loader_batch_size\": VAE_BATCH_SIZE, \n",
    "                        \"clip_guiding_rec\": False,\n",
    "                        \"guiding_clip_min\": 0,\n",
    "                        \"guiding_clip_max\": 100}\n",
    "\n",
    "# Establish sensitivity prediction network config\n",
    "sensitivity_prediction_network_config = {\"layers\": (DRUG_LATENT_DIM + CELL_LINE_LATENT_DIM, 512, 256, 128, 1),\n",
    "                                        \"learning_rate\": 0.0005,\n",
    "                                        \"l2_term\": 0,\n",
    "                                        \"dropout_rate1\": 0.5,\n",
    "                                        \"dropout_rate2\": 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb22d0c4-11bf-4335-93dc-fd20dedf9d92",
   "metadata": {},
   "source": [
    "## Run the model multiple times with different data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f36a89b7-eb26-41f3-a093-2f38e0ae43af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 11\n",
      "/home/adam/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/adam/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1764: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: final_runs\\GMM_VAE__IP__no_comps=3__trained_comp_std/run_0_split_seed_11\n",
      "\n",
      "  | Name                           | Type                                      | Params\n",
      "---------------------------------------------------------------------------------------------\n",
      "0 | drug_model                     | GMMVAE                                    | 142 K \n",
      "1 | cell_line_model                | AutoencoderConfigurable                   | 80.0 K\n",
      "2 | sensitivity_prediction_network | FeedForwardThreeLayersConfigurableDropout | 175 K \n",
      "---------------------------------------------------------------------------------------------\n",
      "397 K     Trainable params\n",
      "0         Non-trainable params\n",
      "397 K     Total params\n",
      "1.592     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/adam/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1552 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "You are trying to `self.log()` but it is not managed by the `Trainer` control flow",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMisconfigurationException\u001B[0m                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [6], line 73\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# Establish trainer\u001B[39;00m\n\u001B[1;32m     70\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(max_epochs\u001B[38;5;241m=\u001B[39mNUM_EPOCHS, logger\u001B[38;5;241m=\u001B[39mtb_logger, gpus\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, \n\u001B[1;32m     71\u001B[0m                      callbacks\u001B[38;5;241m=\u001B[39m[freezing_callback, checkpoint_callback])\n\u001B[0;32m---> 73\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# Save hyperparams\u001B[39;00m\n\u001B[1;32m     76\u001B[0m whole_model_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvae_var_transformation\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(var_transformation)\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:696\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    677\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;124;03mRuns the full optimization routine.\u001B[39;00m\n\u001B[1;32m    679\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    693\u001B[0m \u001B[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001B[39;00m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 696\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    698\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:650\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    648\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    649\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 650\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    651\u001B[0m \u001B[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001B[39;00m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:735\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    731\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[1;32m    732\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_ckpt_path(\n\u001B[1;32m    733\u001B[0m     ckpt_path, model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    734\u001B[0m )\n\u001B[0;32m--> 735\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1166\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1162\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mrestore_training_state()\n\u001B[1;32m   1164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[0;32m-> 1166\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1168\u001B[0m log\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1169\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1252\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1250\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[1;32m   1251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[0;32m-> 1252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1283\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mtrainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   1282\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1283\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:200\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 200\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:271\u001B[0m, in \u001B[0;36mFitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher\u001B[38;5;241m.\u001B[39msetup(\n\u001B[1;32m    268\u001B[0m     dataloader, batch_to_device\u001B[38;5;241m=\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_strategy_hook, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_to_device\u001B[39m\u001B[38;5;124m\"\u001B[39m, dataloader_idx\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    269\u001B[0m )\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 271\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:200\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 200\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:203\u001B[0m, in \u001B[0;36mTrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_started()\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 203\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[1;32m    207\u001B[0m \u001B[38;5;66;03m# update non-plateau LR schedulers\u001B[39;00m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:200\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 200\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:87\u001B[0m, in \u001B[0;36mTrainingBatchLoop.advance\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[1;32m     84\u001B[0m     optimizers \u001B[38;5;241m=\u001B[39m _get_active_optimizers(\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39moptimizer_frequencies, kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     86\u001B[0m     )\n\u001B[0;32m---> 87\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     89\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_loop\u001B[38;5;241m.\u001B[39mrun(kwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:200\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 200\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:201\u001B[0m, in \u001B[0;36mOptimizerLoop.advance\u001B[0;34m(self, optimizers, kwargs)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madvance\u001B[39m(\u001B[38;5;28mself\u001B[39m, optimizers: List[Tuple[\u001B[38;5;28mint\u001B[39m, Optimizer]], kwargs: OrderedDict) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[1;32m    199\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_kwargs(kwargs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer_idx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hiddens)\n\u001B[0;32m--> 201\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_optimization\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim_progress\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_position\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001B[39;00m\n\u001B[1;32m    204\u001B[0m         \u001B[38;5;66;03m# would be skipped otherwise\u001B[39;00m\n\u001B[1;32m    205\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer_idx] \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39masdict()\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:248\u001B[0m, in \u001B[0;36mOptimizerLoop._run_optimization\u001B[0;34m(self, kwargs, optimizer)\u001B[0m\n\u001B[1;32m    240\u001B[0m         closure()\n\u001B[1;32m    242\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    247\u001B[0m     \u001B[38;5;66;03m# the `batch_idx` is optional with inter-batch parallelism\u001B[39;00m\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_idx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    250\u001B[0m result \u001B[38;5;241m=\u001B[39m closure\u001B[38;5;241m.\u001B[39mconsume_result()\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;66;03m# if no result, user decided to skip optimization\u001B[39;00m\n\u001B[1;32m    254\u001B[0m     \u001B[38;5;66;03m# otherwise update running loss + reset accumulated loss\u001B[39;00m\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;66;03m# TODO: find proper way to handle updating running loss\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:358\u001B[0m, in \u001B[0;36mOptimizerLoop._optimizer_step\u001B[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_ready()\n\u001B[1;32m    357\u001B[0m \u001B[38;5;66;03m# model hook\u001B[39;00m\n\u001B[0;32m--> 358\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moptimizer_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m    \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_step_and_backward_closure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m    \u001B[49m\u001B[43mon_tpu\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTPUAccelerator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_native_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mamp_backend\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mAMPType\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mNATIVE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m    \u001B[49m\u001B[43musing_lbfgs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_lbfgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_accumulate:\n\u001B[1;32m    371\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_completed()\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1550\u001B[0m, in \u001B[0;36mTrainer._call_lightning_module_hook\u001B[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1547\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[1;32m   1549\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1550\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1553\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/core/module.py:1705\u001B[0m, in \u001B[0;36mLightningModule.optimizer_step\u001B[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001B[0m\n\u001B[1;32m   1623\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimizer_step\u001B[39m(\n\u001B[1;32m   1624\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1625\u001B[0m     epoch: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1632\u001B[0m     using_lbfgs: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1633\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1634\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1635\u001B[0m \u001B[38;5;124;03m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001B[39;00m\n\u001B[1;32m   1636\u001B[0m \u001B[38;5;124;03m    each optimizer.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1703\u001B[0m \n\u001B[1;32m   1704\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1705\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_closure\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168\u001B[0m, in \u001B[0;36mLightningOptimizer.step\u001B[0;34m(self, closure, **kwargs)\u001B[0m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 168\u001B[0m step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_after_step()\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m step_output\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:216\u001B[0m, in \u001B[0;36mStrategy.optimizer_step\u001B[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;124;03m\"\"\"Performs the actual optimizer step.\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \n\u001B[1;32m    208\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;124;03m    **kwargs: Any extra arguments to ``optimizer.step``\u001B[39;00m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    215\u001B[0m model \u001B[38;5;241m=\u001B[39m model \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\n\u001B[0;32m--> 216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecision_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153\u001B[0m, in \u001B[0;36mPrecisionPlugin.optimizer_step\u001B[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[1;32m    152\u001B[0m     closure \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001B[0;32m--> 153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m():\n\u001B[0;32m---> 28\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/torch/optim/adam.py:92\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m---> 92\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n\u001B[1;32m     95\u001B[0m     params_with_grad \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:138\u001B[0m, in \u001B[0;36mPrecisionPlugin._wrap_closure\u001B[0;34m(self, model, optimizer, optimizer_idx, closure)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_closure\u001B[39m(\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    127\u001B[0m     model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    130\u001B[0m     closure: Callable[[], Any],\n\u001B[1;32m    131\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001B[39;00m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001B[39;00m\n\u001B[1;32m    134\u001B[0m \n\u001B[1;32m    135\u001B[0m \u001B[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001B[39;00m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 138\u001B[0m     closure_result \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_closure(model, optimizer, optimizer_idx)\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m closure_result\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:146\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:132\u001B[0m, in \u001B[0;36mClosure.closure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclosure\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClosureResult:\n\u001B[0;32m--> 132\u001B[0m     step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarning_cache\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:407\u001B[0m, in \u001B[0;36mOptimizerLoop._training_step\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001B[39;00m\n\u001B[1;32m    399\u001B[0m \n\u001B[1;32m    400\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;124;03m    A ``ClosureResult`` containing the training step output.\u001B[39;00m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    406\u001B[0m \u001B[38;5;66;03m# manually capture logged metrics\u001B[39;00m\n\u001B[0;32m--> 407\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()\n\u001B[1;32m    410\u001B[0m model_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step_end\u001B[39m\u001B[38;5;124m\"\u001B[39m, training_step_output)\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1704\u001B[0m, in \u001B[0;36mTrainer._call_strategy_hook\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1701\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1703\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1704\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1706\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1707\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:358\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mtrain_step_context():\n\u001B[1;32m    357\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, TrainingStep)\n\u001B[0;32m--> 358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/vadeers/code/gmm-vae-compounds/models/sens_model_gmm_vae.py:73\u001B[0m, in \u001B[0;36mSensitivityModelGMMVAE.training_step\u001B[0;34m(self, train_batch, batch_idx)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvae_dataloader \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvae_training_step_rate \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 73\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_vae\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# Unpack batch data\u001B[39;00m\n\u001B[1;32m     76\u001B[0m batch_vae_input_X, batch_vae_guiding_X, batch_vae_guiding_classes, batch_aen_input_X, batch_targets, drug_ids, cell_line_ids \u001B[38;5;241m=\u001B[39m train_batch   \u001B[38;5;66;03m# SUSPECTLIBLE TO CHANGES\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/vadeers/code/gmm-vae-compounds/models/sens_model_gmm_vae.py:120\u001B[0m, in \u001B[0;36mSensitivityModelGMMVAE.train_vae\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvae_training_num_epochs):\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvae_dataloader:\n\u001B[0;32m--> 120\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrug_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m         \u001B[38;5;66;03m# clear gradients\u001B[39;00m\n\u001B[1;32m    122\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/Projects/vadeers/code/gmm-vae-compounds/models/gmm_vae.py:155\u001B[0m, in \u001B[0;36mGMMVAE.training_step\u001B[0;34m(self, train_batch, batch_idx)\u001B[0m\n\u001B[1;32m    153\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_rec_likelihood_weight \u001B[38;5;241m*\u001B[39m neg_input_rec_likelihood \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mguiding_rec_likelihood_weight \u001B[38;5;241m*\u001B[39m neg_guiding_rec_likelihood \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentropy_weight \u001B[38;5;241m*\u001B[39m entropy \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgmm_likelihood_weight \u001B[38;5;241m*\u001B[39m gmm_likelihood\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 155\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_rec_likelihood\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneg_input_rec_likelihood\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprog_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mguiding_rec_likelihood\u001B[39m\u001B[38;5;124m\"\u001B[39m, neg_guiding_rec_likelihood, on_step\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, prog_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m, entropy, on_step\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, prog_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/miniconda3/envs/hgraph2graph/lib/python3.8/site-packages/pytorch_lightning/core/module.py:436\u001B[0m, in \u001B[0;36mLightningModule.log\u001B[0;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001B[0m\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\n\u001B[1;32m    431\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to `self.log()` but the loop\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms result collection is not registered\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    432\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m yet. This is most likely because you are trying to log in a `predict` hook,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    433\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt support logging\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    434\u001B[0m     )\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 436\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to `self.log()` but it is not managed by the `Trainer` control flow\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    438\u001B[0m     )\n\u001B[1;32m    440\u001B[0m on_step, on_epoch \u001B[38;5;241m=\u001B[39m _FxValidator\u001B[38;5;241m.\u001B[39mcheck_logging_and_get_default_levels(\n\u001B[1;32m    441\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_fx_name, on_step\u001B[38;5;241m=\u001B[39mon_step, on_epoch\u001B[38;5;241m=\u001B[39mon_epoch\n\u001B[1;32m    442\u001B[0m )\n\u001B[1;32m    444\u001B[0m \u001B[38;5;66;03m# make sure user doesn't introduce logic for multi-dataloaders\u001B[39;00m\n",
      "\u001B[0;31mMisconfigurationException\u001B[0m: You are trying to `self.log()` but it is not managed by the `Trainer` control flow"
     ]
    }
   ],
   "source": [
    "# Data split seeds\n",
    "SPLIT_SEEDS = [11, 13, 26, 76, 92]\n",
    "\n",
    "# Data split and loaders hyperparameters\n",
    "NUM_TEST_CELL_LINES = 100\n",
    "BATCH_SIZE_TRAIN = 128\n",
    "BATCH_SIZE_TEST = 512\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 200\n",
    "SAVE_CHECKPOINT_EVERY_N_EPOCHS = 10\n",
    "FREEZE_EPOCH = 150\n",
    "AFTER_FREEZE_LR = 0.001\n",
    "STEP_SIZE = 10   # Step for learning rate shrinkage\n",
    "GAMMA = 0.1   # Shrinkage factor for learning rate\n",
    "\n",
    "for exp_run, split_seed in enumerate(SPLIT_SEEDS):\n",
    "    dataset_train, dataset_test, train_cell_lines, test_cell_lines = full_dataset.train_test_split(NUM_TEST_CELL_LINES, seed=split_seed,\n",
    "                                                                                              return_cell_lines=True)\n",
    "    # Create corresponding DataLoaders\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE_TEST)\n",
    "    \n",
    "    pl.utilities.seed.seed_everything(split_seed)\n",
    "    \n",
    "    # Establish drug model\n",
    "    drug_gmm_vae = GMMVAE(whole_model_config[\"drug_encoder_layers\"], whole_model_config[\"drug_input_decoder_layers\"], \n",
    "                          whole_model_config[\"drug_guiding_decoder_layers\"], \n",
    "                          whole_model_config[\"no_gmm_components\"],\n",
    "                          components_std=whole_model_config[\"components_std\"],\n",
    "                          var_transformation=var_transformation, \n",
    "                          learning_rate=whole_model_config[\"drug_model_learning_rate\"],\n",
    "                          loss_function_weights=whole_model_config[\"vae_loss_function_weights\"], \n",
    "                          batch_norm=False, optimizer=\"adam\",\n",
    "                          encoder_dropout_rate=0, decoders_dropout_rate=0,\n",
    "                          clip_guiding_rec=whole_model_config[\"clip_guiding_rec\"],\n",
    "                          guiding_clip_min=whole_model_config[\"guiding_clip_min\"],\n",
    "                          guiding_clip_max=whole_model_config[\"guiding_clip_max\"])\n",
    "    \n",
    "    # Set up trainable componenst stds - comment below line if you want to have fixed isotropic covariance\n",
    "    # matrices in GMM\n",
    "    drug_gmm_vae.stds = nn.Parameter(data=torch.ones(whole_model_config[\"no_gmm_components\"], drug_gmm_vae.latent_dim), requires_grad=True)\n",
    "\n",
    "    # Establish cell line model\n",
    "    cell_line_aen = modules.AutoencoderConfigurable(whole_model_config[\"cell_line_encoder_layers\"], whole_model_config[\"cell_line_decoder_layers\"])\n",
    "\n",
    "    # Three-layer variant\n",
    "    sensitivity_prediction_network = modules.FeedForwardThreeLayersConfigurableDropout(sensitivity_prediction_network_config)\n",
    "    \n",
    "    # Assemble the model\n",
    "    model = SensitivityModelGMMVAE(drug_gmm_vae, cell_line_aen, sensitivity_prediction_network,\n",
    "                                  learning_rate=whole_model_config[\"learning_rate\"],\n",
    "                                  aen_reconstruction_loss_weight=whole_model_config[\"aen_reconstruction_weight\"],\n",
    "                                  sensitivity_loss_weight=whole_model_config[\"sensitivity_prediction_weight\"],\n",
    "                                  vae_dataloader=vae_dataloader) # to na None, wtedy nie ma traning dodatkowego i OK\n",
    "   \n",
    "    # Train the model\n",
    "    # Establish logger\n",
    "    model_name = f\"\"\"GMM_VAE__IP__no_comps={NO_GMM_COMPONENTS}__trained_comp_std\"\"\"\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(rf\"final_runs\\{model_name}\", name=f\"run_{exp_run}_split_seed_{split_seed}\")\n",
    "    \n",
    "    # Establish callbacks\n",
    "    freezing_callback = utils.FreezingCallback(freeze_epoch=FREEZE_EPOCH, new_learning_rate=AFTER_FREEZE_LR, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "    \n",
    "    # Overwrite default checkpoint callback if needed\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_sensitivity_pred_rmse\", every_n_epochs=SAVE_CHECKPOINT_EVERY_N_EPOCHS, every_n_train_steps=None, train_time_interval=None,\n",
    "                                         save_top_k=NUM_EPOCHS // SAVE_CHECKPOINT_EVERY_N_EPOCHS)\n",
    "\n",
    "    # Establish trainer\n",
    "    trainer = pl.Trainer(max_epochs=NUM_EPOCHS, logger=tb_logger, gpus=0, \n",
    "                         callbacks=[freezing_callback, checkpoint_callback])\n",
    "\n",
    "    trainer.fit(model, dataloader_train, dataloader_test)\n",
    "\n",
    "    # Save hyperparams\n",
    "    whole_model_config[\"vae_var_transformation\"] = str(var_transformation)\n",
    "    whole_model_config[\"num_epochs\"] = NUM_EPOCHS\n",
    "    whole_model_config[\"freeze_epoch\"] = FREEZE_EPOCH\n",
    "    whole_model_config[\"after_freeze_lr\"] = AFTER_FREEZE_LR\n",
    "\n",
    "    with open(os.path.join(trainer.log_dir, \"whole_model_config.json\"), \"w\") as f:\n",
    "        json.dump(whole_model_config, f)\n",
    "\n",
    "    with open(os.path.join(trainer.log_dir, \"sensitivity_prediction_network_config.json\"), \"w\") as f:\n",
    "        json.dump(sensitivity_prediction_network_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
